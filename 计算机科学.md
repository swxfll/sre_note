# 常见线程模型

## 1 进程和线程

进程

*   PCB
*   数据段
*   程序段

线程

*   TCB
*   数据段
*   程序段

**进程是资源分配的基本单位, 线程是可执行的基本单位, 是可被调度的基本单位**
**线程不可以自己独立拥有资源, 线程的执行必须依赖于所属进程中的资源,进程中必须至少应该有一个线程**

以现代标准而言，一个标准PC的操作系统应该提供以下的功能：

*   进程管理（Processing management）
    *   在进程之下尚有线程的问题，但是大部分的操作系统并不会处理线程所遭遇的问题，通常操作系统仅止于提供一组API让用户自行操作或透过虚拟机的管理机制控制线程之间的交互。
*   内存管理（Memory management）
    *   大部分的现代电脑存储器架构都是层次结构式的，最快且数量最少的**寄存器**为首，然后是缓存、存储器以及最慢的磁盘存储设备。
*   文件系统（File system）
*   网络通信（Networking）
*   安全机制（Security）
*   用户界面（User interface）
*   驱动程序（Device drivers）

操作系统之本意原为提供简单的工作排序能力，后为辅助更新更复杂的硬件设施而渐渐演化。

总而言之，操作系统的历史就是一部解决电脑系统需求与问题的历史。

狭义上的操作系统, 通常指的是运行在裸硬件(裸金属)上的,用于抽象并管理硬件资源的软件.维基百科对此进行了定义:操作系统(Operating Systemc, 缩写: OS)是一组主管并控制计算机操作、运用和运行硬件、软件资源和提供公共服务来组织用户交互的相互关联的系统软件程序,同时也是计算机系统的核心与基石.操作系统需要处理如管理与配置内存、决定系统资源供需的优先次序、控制输入与输出装置、操作网络与管理文件系统等基本事务.操作系统也提供一个让使用者与系统互动的操作页面.

⼴义上的操作系统，Kubernetes也算是操作系统的⼀员。操作系统的设计与实现是⼯业软件（AutoCAD、Maya、Matlab、MySQL）的基础和内核。通常我们认为，操作系统指的是内核，⽽类似于Bash、Finder 等软件属于操作系统上提供的软件。操作系统内核需要接触到各式各样的硬件设备，⽽内核开发者不可能涵盖到所有的硬件，尤其是在这个硬件层出不穷的时代。因此需要将内核与硬件设备进⾏解耦，这就是驱动程序的由来。内核提供⼀系列的协议，驱动程序需要遵守这些协议，然后转化为指定硬件的操作。

通常这个"操作页面"分为图形化操作页面和终端操作页面.常见的图形化操作系统有macOS和Windows,Linux和UNIX存在众多的发行版,有的用于开发板,通常使用串口作为操作页面;有的用于工作站,通常厂商会开发一个图形化的桌面;有的用于服务器,通常使用网络连接作为操作页面.

在内核上会有大量的软件,如果所有的软件均直接运行在内核上,那么内核将变得不安全.现代的内核通常提供两种模式:**内核模式**和**用户模式**.软件和一部分驱动工作在用户模式中, 一些必要的驱动直接工作在内核模式.内核的设计从一开始的单任务发展到多任务,例如Linux的内核提供了`task_struct`这个数据结构描述任务.但是创造内核任务是昂贵的,因为内核需要维护**上下文**并且负责任务的调度.而现代软件通常是多个任务同时执行的,因此用户模式下的任务变得受欢迎.从此,进程变为一个划分资源的单位,例如一个Bash登录环境后就会创建一个进程,并为此分配内存和处理器资源.

> 上下文
> 上下文在不同的地方表示不同的含义
> 看，一篇文章，给你摘录一段，没前没后，你读不懂，因为有语境，就是语言环境存在，一段话说了什么，要通过上下文(文章的上下文)来推断。
> 子程序之于程序，进程之于操作系统，甚至app的一屏之于app，都是一个道理。
> 程序执行了部分到达子程序，子程序要获得结果，要用到程序之前的一些结果(包括但不限于外部变量值，外部对象等等)；app点击一个按钮进入一个新的界面，也要保存你是在哪个屏幕跳过来的等等信息，以便你点击返回的时候能正确跳回，如果不存肯定就无法正确跳回了。看这些都是上下文的典型例子，理解成环境就可以，
> context是environment的snapshot.
> [什么是上下文](https://juejin.cn/post/6844903954816434190)
> [编程中什么是「Context(上下文)」？](https://www.zhihu.com/question/26387327/answer/32618592)

由于需要运行很多的任务,那么在**Linux2.4版本**之前,内核没有提供线程的概念.人们通常写一个线程库来模拟线程.让多个线程在一个进程资源空间中运行,所以说线程是任务(调度单位、代码块)的最小单位.由于各家针对的线程的实现风格迥异,而且也只是模拟,多个线程针对内核还是一个进程,**并不能运行到多核心上**.想要实现真正的线程,对内核的改动是非常大的,RedHat和IBM分别给出了方案:

*   RedHat 提供了 NTPL(Native POSIX Thread Library)项目
*   IBM 提供了NGTP(Next Generation POSIX Threads)项目

后来由于各种原因,IBM放弃了该项目,至此,RedHat NPTL 项目成为 Linux 默认的线程库.各种编程语言也提供了很多任务管理的库,Linux上常见的有C/C++ -- Glibc.Glibc是GNU发布的C/C++运行时库,几乎任何库都依赖于Glibc,比如JVM的线程通过Native方式创建任务.人们通过NPTL调用创建线程的函数创造线程,这些线程都是内核级别的线程.与进程一样,内核级别的线程通常也是昂贵的.面对高并发和大量的客户端应用频繁的创建和销毁内核线程使得内核不堪重负.为了**尽最大力度去复用内核线程**,人们发明了用户线程.那么,那就产生了一个问题:**用户线程使用怎么样的方式映射到内核线程?**

在现实世界中,人们提供了三种方式:

*   1:1(内核级线程模型)
*   1\:N(用户级线程模型)
*   M\:N(混合式线程模型)

**注意:内核线程通常称为KSE(Kernel Scheduling Entity, 内核调度实体).这些KSE参与CPU时间片的瓜分,多个KSE在一个CPU核心上运行就是并发,多个KSE在多个CPU核心上运行就是并行**

### 1.1 用户线程映射到内核线程的方式

#### 1:1(内核级线程模型)

![image](https://assetsasda.oss-cn-guangzhou.aliyuncs.com/devops/%E5%86%85%E6%A0%B8%E7%BA%A7%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B.png)
将用户线程以1:1的方式映射到内核线程上.用户线程阻塞之后不会影响其他线程的执行,而且可以让多线程程序拥有更好的表现.许多操作系统限制了内核线程的数量,所以间接导致用户线程受损.大部分编程语言的线程库如\:Linux的`pthread`、java的`java.lang.Thread`和C++的`std::thread`都是包装了操作系统的线程.其调度完全是操作系统线程调度器来做.

由于操作系统内核直接创建线程、销毁线程,并且还需要维护线程的上下文信息,因此资源成本大幅度上升.内核级线程模型在多处理器架构上,内核能够并行执行同一个进程中的多个线程,如果发生阻塞直接切换同一个进程中的其他线程执行.该模型的另一个缺点在于所有阻塞线程的操作都是以系统调用的形式进行的.

#### N:1(用户级线程模型)

![image](https://assetsasda.oss-cn-guangzhou.aliyuncs.com/devops/%E7%94%A8%E6%88%B7%E7%BA%A7%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B.png)

将用户线程以 N:1 的方式映射到内核线程上.语言运行时通常会实现一个线程调度器,线程之间的切换由语言运行时实现.内核感受不到线程的切换和实现,内核的所有调度都是基于用户进程的. Python的gevent协程库实现采用该模型.该模型对系统资源较小,整体上来说较为轻量级.

**最大的缺点在于不是真正意义上的并发**.某个用户线程由于I/O阻塞或者系统调用阻塞会导致整个用户进程阻塞,因为用户进程中的线程没有接收CPU中断的能力,属于自调度

在用户级线程模型下,多处理器架构也只是用户进程关联到一个CPU,很多协程库将一些阻塞的操作封装成非阻塞的,在阻塞代码段让出CPU时间片并通过某种手段通知其他用户线程运行,避免内核调度器由于内核线程阻塞做**上下文**切换,解决整个用户进程阻塞问题.

该模型解决了某些操作系统不支持线程的情况,用户线程的状态级切换和上下文托管都在用户进程中实现,调度算法不受操作系统限制.用户线程模型能够利用较多的堆栈空间,并且不需要系统调用和系统内核上下文切换,更不需要刷新内存高速缓存,使得用户线程的调度速度很快.[用户级线程线程的存在意义是什么?](https://www.zhihu.com/question/307787570/answer/592442800)

#### N\:M 混合式线程模型

![image](https://assetsasda.oss-cn-guangzhou.aliyuncs.com/devops/%E6%B7%B7%E5%90%88%E5%BC%8F%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B.png)

将用户线程以N\:M的方式映射到内核线程上
语言运行时调度器和内核都参与工作

进程和线程的元数据维护在栈空间,进程栈空间执行时是确定的,与编译链接无关.进程栈空间的大小是随机的,并且要比线程栈空间高出2倍.线程栈空间也是确定的,可以使用`ulimit -a`查看,使用`ulimit -s`修改.一般默认情况下,`线程栈在进程的堆中分配`,每个线程拥有独立的栈空间,为了避免线程之间的栈空间踩踏,线程栈之间还会以小块 Guard Size 用来隔离保护各自的栈空间(4K大小的保护页, 防止栈溢出),一旦另一个线程踏入到这个隔离区,就会引发段错误.

# 2 SMP NUMA MPP

目前, 商用服务器处理架构有三种: SMP(Symmetric Multi-Processor, 对称多处理器结构)、NUMA(Non-Uniform Memory Access, 非一致存储访问结构)和MPP(Massive Parallel Processing, 海量并行处理结构)

为什么需要多处理器架构?主要在于三个优点:

1.  增加吞吐量:通过增加处理器数量,以期望在更短的时间内完成工作.采用N个处理器的加速并不等于N,而是接近于N.当多个处理器协同完成同一个任务时,会存在一定的额外开销.
2.  规模经济:多处理器协同的价格远低于相同负载的单元处理器系统,多处理器系统可以共享大量的外设、大容量的存储存储和电源.如果多个程序同时操作一个数据集,那么将这些数据放在同一个磁盘并让多处理器共享出力,这要比拷贝副本更加节省.
3.  增加可靠性:如果将功能分布在多个处理器上,那么单处理器的故障不会使得整个系统停止运行.单处理器系统崩溃对于数据中心是致命的.

## 2.1 SMP(Symmetric Multi-Processor, 对称多处理器结构)

SMP是相对于⾮对称多处理器技术应⽤最⼴泛的并⾏计算技术。在 SMP 中，所有处理器的地位都是平等的，通过总线连接到同⼀个共享的物理
内存，硬件系统中的所有资源都是共享的。SMP 应⽤⼴泛的主要原因在于⼤部分的
⼿机、笔记本都在使⽤该架构，并且⼀些较为⽼旧的服务器也使⽤这种架构。对于
SMP 架构的服务器来说，每⼀个共享环节都并发的访问硬件资源，这就导致当
CPU 数量增加后，资源的抢占不断增加。
实验证明：SMP 架构的服务器 CPU 利⽤率最好的情况是 2\~4 个 CPU。

![image](https://assetsasda.oss-cn-guangzhou.aliyuncs.com/devops/SMP_-_Symmetric_Multiprocessor_System.svg.png)

## 2.2 NUMA(Non-Uniform Memory Access, 非一致存储访问结构)

NUMA 的提出在于 SMP 扩展能⼒的限制。不过 NUMA 架构解决横向扩展能⼒的同时带来了**访问延迟**。NUMA 将资源划分，提供了 Node（节点）的概念。
NUMA 使⽤ Node 的概念将硬件资源进⾏分割，每个 Node 都有独有的处理器核⼼、内存和 I/O 资源。这样做的依据就是访问本地资源的速度远远⾼于其他资源，但是同样的问题在于多个 Node 之间资源交互⾮常慢，当 CPU 增多的时候，性能提升幅度并不是很⾼。
利⽤ NUMA 技术，可以较好地解决原来 SMP 系统的扩展问题，在⼀个物理服务器内可以⽀持上百个 CPU 。
![image](https://assetsasda.oss-cn-guangzhou.aliyuncs.com/devops/numa1.png)
![image](https://assetsasda.oss-cn-guangzhou.aliyuncs.com/devops/numa2.png)

## 2.3 MPP(Massive Parallel Processing, 海量并行处理结构)

⼤规模并⾏处理系统，这样的系统是由许多松耦合的处理单元组成的，要注意的是这⾥指的是处理单元⽽不是处理器。每个单元内的CPU都有⾃⼰私有的资源，如总线，内存，硬盘等。在每个单元内都有操作系统和管理数据库的实例复本。这种结构最⼤的特点在于不共享资源。

由多个SMP服务器通过⼀定的节点互联⽹络进⾏连接（信息交互的过程⼀般称为数据重分配），协同⼯作，完成相同的任务，从⽤户的⻆度来看是⼀个服务器系统。

理论上其扩展⽆限制，⽬前的技术可实现 512 个节点互联，数千个 CPU 。在MPP 系统中，每个 SMP 节点也可以运⾏⾃⼰的操作系统、数据库等。MPP 服务器需要⼀种复杂的机制来调度和平衡各个节点的负载和并⾏处理过程。

⽬前⼀些基于 MPP 技术的服务器往往通过系统级软件 ( 如数据库 ) 来屏蔽这种复杂性。

## 2.4 三种多处理器架构的应⽤场景

*   MPP系统不共享资源，因此对它⽽⾔，资源⽐SMP要多，当需要处理的事务达到⼀定规模时，MPP的效率要⽐SMP好。由于MPP系统因为要在不同处理单元之间传送信息，在通信时间少的时候，那MPP系统可以充分发挥资源的优势，达到⾼效率。也就是说：操作相互之间没有什么关系，处理单元之间需要进⾏的通信⽐较少，那采⽤MPP系统就要好。因此，MPP系统在决策⽀持和数据挖掘⽅⾯显示了优势。

*   MPP系统因为要在不同处理单元之间传送信息，所以它的效率要⽐SMP要差⼀点。在通讯时间多的时候，那MPP系统可以充分发挥资源的优势。因此当前使⽤的OLTP程序中，⽤户访问⼀个中⼼数据库，如果采⽤SMP系统结构，它的效率要⽐采⽤MPP结构要快得多。

*   NUMA架构来看，它可以在⼀个物理服务器内集成许多CPU，使系统具有较⾼的事务处理能⼒，由于远地内存访问时延远⻓于本地内存访问，因此需要尽量减少不同CPU模块之间的数据交互。显然，NUMA架构更适⽤于OLTP事务处理环境，当⽤于数据仓库环境时，由于⼤量复杂的数据处理必然导致⼤量的数据交互，将使CPU的利⽤率⼤⼤降低。

### 2.4.1 什么是 OLTP?
OLTP 或联机事务处理是一种数据处理类型，包括执行多个并发的事务，例如网上银行、购物、订单输入或发送文本消息。这些事务传统上被称为经济或财务事务，会被记录并加以保护，帮助企业随时访问这些信息，以用于会计或报告目的。

过去，OLTP 仅限于交换金钱、产品、信息、服务请求等某些东西的实际交互。但多年来，这种情景下事务的定义已经扩大，尤其是自互联网问世以来，它涵盖了可以从世界上任何地方，通过任何网络连接的传感器触发的任何类型数字交互或与企业的互动。它还包括任何类型的交互或操作，例如在网页上下载 PDF、查看特定视频或社交渠道上的自动维护触发器或备注；这些触发器或评论可能对于企业记录以更好地为客户提供服务至关重要。

# 3 ARM 处理器、新型的内存、⽹卡、硬盘与 FPGA 设备

## [ARM 架构简介](https://www.arm.com/en/architecture/cpu)

# [4 缓存⼀致性协议 MESI](https://www.cnblogs.com/xmzJava/p/11417943.html)

MESI（Modified-Exclusive-Shared-Invalid）协议是一种广为使用的缓存一致性协议。MESI协议对内存数据访问的控制类似于读写锁，它使得针对同一地址的读内存操作是并发的，而针对同一地址的写内存操作是独占的。

## [4.1 CPU为何要有高速缓存](https://cloud.tencent.com/developer/article/1548942)

![image](https://assetsasda.oss-cn-guangzhou.aliyuncs.com/devops/%E5%AD%98%E5%82%A8%E5%99%A8%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84.jpg)

CPU 通常在内存存取数据，由于 CPU 的运算核⼼通常⽐内存速度⾼出⼏个数量级，为了解决这个问题，CPU厂商在CPU中内置了少量的高速缓存以解决I\O速度和CPU运算速度之间的不匹配问题。

局部性原理: 在CPU访问存储设备时，⽆论是存取数据抑或存取指令，都趋于聚集在⼀⽚连续的区域中，这就被称为局部性原理。

*   时间局部性: 如果⼀个信息项正在被访问，那么在近期它很可能还会被再次访问。(倾向于引用最近引用过的数据。)
*   空间局部性: 如果⼀个存储器的位置被引⽤，那么将来他附近的位置也会被引⽤。(倾向于引用最近引用过的数据附近的数据。)

这些结论都是⼈们从⼤量的⽣产实践观察总结出来的。

当 CPU 中设计了⾼速缓存，那么 CPU 执⾏指令的流程为:

1.  程序以及数据加载到主内存
2.  指令和数据加载到 CPU 的⾼速缓存
3.  CPU 执⾏指令，把结果写到⾼速缓存中，
4.  高速缓存中的数据写回主内存

![image](https://assetsasda.oss-cn-guangzhou.aliyuncs.com/devops/%E5%B8%A6%E6%9C%89%E9%AB%98%E9%80%9F%E7%BC%93%E5%AD%98%E7%9A%84CPU%E6%89%A7%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%9A%84%E6%B5%81%E7%A8%8B.jpeg)

由于CPU的运算速度超越了1级缓存的数据I\O能⼒，CPU⼚商⼜引⼊了多级的缓存结构。
![image](https://assetsasda.oss-cn-guangzhou.aliyuncs.com/devops/%E7%9B%AE%E5%89%8D%E6%B5%81%E8%A1%8C%E7%9A%84%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98%E7%BB%93%E6%9E%84.jpeg)

**多核CPU多级缓存一致性协议MESI**
多核CPU的情况下有多个一级缓存，如何保证缓存内部数据的一致,不让系统数据混乱。这里就引出了一个一致性的协议MESI。

**MESI 协议缓存状态**
**缓存行（Cache line）**:缓存存储数据的单元。
MESI是指4种状态的首字母。每个Cache line有4个状态，可用2个bit表示，它们分别是：

| 状态                 | 描述                                  | 监听任务                                                            | 状态转变                                            |
| ------------------ | ----------------------------------- | --------------------------------------------------------------- | ----------------------------------------------- |
| M 修改 (Modified     | 该缓存行有效,数据被修改了,和内存中的数据不一致,数据只存在于本缓存中 | 缓存行必须时刻监听所有试图读该缓存行相对就主存的操作,这种操作必须在缓存将该缓存行写回主存并将状态变成S(共享)状态之前被执行 | 当被写回主存之后，该缓存行的状态会变成独享（exclusive)状态。             |
| E 独享、互斥(Exclusive) | 该缓存行有效,数据和内存中的数据一致,数据只存在于本缓存中       | 缓存行必须监听其它缓存读主存中该缓存行的操作,一旦有这种操作,该缓存行需要变成S(共享)状态                  | 当CPU修改该缓存行中内容时，该状态可以变成Modified状态                |
| S 共享(Shared)       | 该缓存行有效,数据和内存中的数据一致,数据存在于很多缓存中       | 缓存行也必须监听其它缓存使该缓存行无效或者独享该缓存行的请求,并将该缓存变成无效(Invalid)               | 当有一个CPU修改该缓存行时，其它CPU中该缓存行可以被作废（变成无效状态 Invalid）。 |
| I 无效(Invalid)      | 该缓存行无效                              | 无                                                               |                                                 |

这些状态本身是静态的，那么动态来看，又是如何产生状态变化的呢？

首先不同CPU之间也是需要沟通的，这里的沟通是通过在消息总线上传递message实现的。这些在总线上传递的消息有如下几种：

*   Read ：带上数据的物理内存地址发起的读请求消息；
*   Read Response：Read 请求的响应信息，内部包含了读请求指向的数据；
*   Invalidate：该消息包含数据的内存物理地址，意思是要让其他如果持有该数据缓存行的 CPU直接失效对应的缓存行；
*   Invalidate Acknowledge：CPU 对Invalidate 消息的响应，目的是告知发起 Invalidate 消息的CPU，这边已经失效了这个缓存行啦；
*   Read Invalidate：这个消息其实是 Read 和 Invalidate 的组合消息，与之对应的响应自然就是一个Read Response 和 一系列的 Invalidate Acknowledge；
*   Writeback：该消息包含一个物理内存地址和数据内容，目的是把这块数据通过总线写回内存里。

![image](https://assetsasda.oss-cn-guangzhou.aliyuncs.com/devops/mesi%E6%B6%88%E6%81%AF%E7%B1%BB%E5%9E%8B.png)

**对于M和E状态⽽⾔总是精确的，他们在和该缓存⾏的真正状态是⼀致的，⽽S状态可能是⾮⼀致的。**  如果⼀个缓存将处于S状态的缓存⾏作废了，⽽另⼀个缓存实际上可能已经独享了该缓存⾏，但是该缓存却不会将该缓存⾏升迁为E状态，这是因为其它缓存不会⼴播他们作废掉该缓存⾏的通知，同样由于缓存并没有保存该缓存⾏的拷⻉的数量，因此（即使有这种通知）也没有办法确定⾃⼰是否已经独享了该缓存⾏。
从上⾯的意义看来E状态是⼀种投机性的优化：如果⼀个CPU想修改⼀个处于S状态的缓存⾏，总线事务需要将所有该缓存⾏的拷⻉变成I状态，⽽修改E状态的缓存不需要使⽤总线事务。

**触发事件**

| 触发事件         | 描述         |
| ------------ | ---------- |
| local Read   | 本地缓存读取本地数据 |
| local Write  | 本地缓写入取本地数据 |
| Remote Read  | 其它缓存读取本地数据 |
| Remote Write | 其它缓存写入本地数据 |

![image](https://assetsasda.oss-cn-guangzhou.aliyuncs.com/devops/mesi%E7%BC%93%E5%AD%98.png)

缓存的⼀致性消息传递是要时间的，这就使其切换时会产⽣延迟。当⼀个缓存被切换状态时其他缓存收到消息完成各⾃的切换并且发出回应消息这么⼀⻓串的时间中CPU都会等待所有缓存响应完成。可能出现的阻塞都会导致各种各样的性能问题和稳定性问题。

⼀次读写操作的演示：

![image](https://assetsasda.oss-cn-guangzhou.aliyuncs.com/devops/mesi%E8%AF%BB%E5%86%99%E6%93%8D%E4%BD%9C.png)

## [4.2 存储缓存（Store Buffer）⽤于解决CPU切换状态时遇到的阻塞问题](https://www.cnblogs.com/xmzJava/p/11417943.html)

⽐如你需要修改本地缓存中的⼀条信息，那么你必须将I（⽆效）状态通知到其他拥有该缓存数据的CPU缓存中，并且等待确认。等待确认的过程会阻塞处理器，这会降低处理器的性能。因为这个等待远远⽐⼀个指令的执⾏时间⻓的多。为了避免这种CPU运算能⼒的浪费，存储缓存被引⼊使⽤。处理器把它想要写⼊到主存的值写到缓存，然后继续去处理其他事情。当所有失效确认（Invalidate Acknowledge）都接收到时，数据才会最终被提交。但是该⽅案也存在两个⻛险：

*   处理器会尝试从存储缓存中读取值，但它还没有进⾏提交。这个的解决⽅案称为 Store Forwarding，它使得加载的时候，如果存储缓存中存在，则进⾏返回。
*   保存什么时候会完成，这个并没有任何保证。

执⾏失效也不是⼀个简单的操作，它需要处理器去处理。另外，存储缓存并不是⽆穷⼤的，所以处理器有时需要等待失效确认的返回。这两个操作都会使得性能⼤幅降低。为了应付这种情况，引⼊了失效队列。它们的约定如下：

*   对于所有的收到的 Invalidate 请求，Invalidate Acknowledge 消息必须⽴刻发送。
*   Invalidate 并不真正执⾏，⽽是被放在⼀个特殊的队列中，在⽅便的时候才会去执⾏。
*   处理器不会发送任何消息给所处理的缓存条⽬，直到它处理 Invalidate。

即便是这样处理器已然不知道什么时候优化是允许的，⽽什么时候并不允许。⼲脆处理器将这个任务丢给了写代码的⼈。这就是内存屏障（Memory Barriers）：

*   写屏障 Store Memory Barrier (a.k.a. ST, SMB, smp\_wmb) 是⼀条告诉处理器在执⾏这之后的指令之前，应⽤所有已经在存储缓存中的保存的指令。
*   读屏障 Load Memory Barrier (a.k.a. LD, RMB, smp\_rmb) 是⼀条告诉处理器在执⾏任何的加载前，先应⽤所有已经在失效队列中的失效操作的指令。

# 5 链表、树、图与字符串

# 6 SQL关系模型与数据库体系结构

## [6.1 数据库三范式](https://segmentfault.com/a/1190000013695030)

### 第一范式

1NF是对属性的**原子性**，要求属性具有原子性，不可再分解；

> 表：字段1、 字段2(字段2.1、字段2.2)、字段3 ......
> 如学生（学号，姓名，性别，出生年月日），如果认为最后一列还可以再分成（出生年，出生月，出生日），它就不是一范式了，否则就是；

### 第二范式

2NF是对记录的**唯一性**，要求记录有唯一标识，即实体的唯一性，即**不存在部分依赖**；

> 表：学号、课程号、姓名、学分
> 这个表明显说明了两个事务:学生信息, 课程信息;由于非主键字段必须依赖主键，这里**学分依赖课程号，姓名依赖与学号**，所以不符合二范式。

**可能会存在问题：**

*   数据冗余: 每条记录都含有相同信息；
*   删除异常：删除所有学生成绩，就把课程信息全删除了；
*   插入异常：学生未选课，无法记录进数据库；
*   更新异常：调整课程学分，所有行都调整。

**正确做法:**

> 学生：Student(学号, 姓名)；
> 课程：Course(课程号, 学分)；
> 选课关系：StudentCourse(学号, 课程号, 成绩)。

### 第三范式

**如果一个关系属于第二范式**,并且在两个(或多个)非主键属性之间不存在函数依赖。(非主键属性之间的函数依赖也称为传递依赖),那么这个关系属于第三范式。

3NF是对字段的冗余性，要求任何字段不能由其他字段派生出来，它要求字段没有冗余，即**不存在传递依赖**；

> 表: 学号, 姓名, 年龄, 学院名称, 学院电话

注意：上表属于第二范式，因为主键由单个属性组成（学号）

因为存在**依赖传递**: (学号) → (学生)→(所在学院) → (学院电话) 。

**可能会存在问题：**

数据冗余:有重复值；
更新异常：有重复的冗余信息，修改时需要同时修改多条记录，否则会出现数据不一致的情况 。

**正确做法：**

> 学生：(学号, 姓名, 年龄, 所在学院)；
> 学院：(学院，学院名称， 电话)。

## 6.2 数据库的基本抽象

数据库系统的构建可以从上到下分为5个层次：

*   查询计划
*   算⼦执⾏
*   访问⽅法
*   缓冲池管理
*   存储管理

其中，存储管理中使⽤的存储器按照现有计算机架构进⾏排序（按照访问速度）

*   CPU寄存器
*   CPU缓存
*   动态内存
*   固态硬盘
*   机械磁盘
*   ⽹络存储

## 6.3 存储管理

**数据库系统的存储引擎就是存储管理系统**

*   数据库中的数据落实到磁盘上,可能是像SQLLite一样的**单文件**,也可能是像PostgreSQL一样的**多文件**; 并且有的数据库系统可能会自研文件系统,但是现代数据库通常不会,因为  这样很难被部署到云厂商的服务器上.
*   数据库通常使用"页"这种概念来描述数据块,并且数据库要求数据是"自托管的",也就是说元数据和数据应该被管理.每个数据页都会有一个全局唯一的PageID.
*   **数据库的元数据存储方式:** 有的数据库的元数据会被单独存储到一个页,数据存储到其他的页,而有的数据库数据和元数据会存储到一个页(独立的页, self-contained page), 因此这有利于**灾备**, 例如Oracle.
*   **操作系统的mmap可能对于数据库系统来说是灾难的**, 例如MongoDB的第一版使用mmap,研发上做了很多无用功.
*   Indirection 层 用于实现数据页映射到某个集合中一个文件的具体位置.PageID可以是数据页在对应文件的相对位置,那么知道整体文件的初始位置和数据页大小就可以知道具体的Offset值.
    *   优点: 如果整体移动数据文件,例如更换硬盘,那么PageID就可以保持不变了
*   页的三种概念:
    *   硬件的页: 硬件存储暴露的组织数据存储的概念,并且是**原子读写**的数据块大小,**通常是4KB**
        *   例如16KB,就可能前8KB数据写入了,由于异常,剩余的8KB也写入了,但是不连续,导致的是损坏的数据.
    *   操作系统的页:从存储设备上取出数据放到内存中的表示,**通常是4KB**.
    *   数据库系统的页: 通常是512B-16KB.
*   在存储引擎级别,我们不关心数据页中到底有什么:
    *   **堆文件,无序且随机**.构建这种文件的方式:
        *   数据页头(页的元数据区)中有**两个指针**,指向Free页链表和Data页链表.
        *   使用**Directory**的概念包装数据页,这个Directory中包含Free页和Data页
    *   顺序/排序文件
    *   哈希文件
*   数据页大小是固定的,并且要比操作系统和硬件的页更大一些.
    *   如果数据页较小,那么一个PageID所表达的数据范围更小,**页表就会导致膨胀**,那么就可能会产生缓存丢失,类似于TLB.
*   由于数据库的页一般要比操作系统和硬件存储的大,那么就出现了一个数据库的页使用随机还是连续的方式落实到下游的页;并且多个下游的数据页如何安全的原子的保存,这样写入数据的代价就会变高,因此商用数据库**允许应用进行调整**
    *   对页的灾备可以使用日志的方式实现
*   在数据页中存储元组,也就是数据库行记录,那么,可以采用顺序存储,但是存在的问题是删除元素会**存储空洞**,因此一个可行的方案是**Slotted Pages**,本质是在数据页头区域设置一个**Slot Array**, 用来记录元组的偏移量.
*   Slotted Pages方案 **无法解决变长元组带来的空间损失**,因此PostgreSQL提供了**Vaccum操作**用于处理这些空间.类似于GC(垃圾回收).
*   元组的布局方式也分为数据区和元数据区.元组中的数据无论是否对齐,数据都会连续存储,这种模型叫做**N-ARY**模型.元组的DML速度很快,但对于TB级别的数据分析通常只需要几列而不是整个元组读出,因此OLAP数据库中有列式存储.
*   另一种存储方式采用**结构化日志**存储数据而不是页.
    *   优点是便于**回滚**且便于操作
    *   缺点是很**难读**,因为追加的是DML语句,也可以经过优化,然后存储被一个DML修改后的数据.
*   如果不想丢失数据的精度,就需要使用**固定的浮点**表示数,但是这需要数据库系统去实现.
*   对于大型的二进制数据,可能的方式是**Overflow Page**或者**外部文件**.
    *   对于移动应用来说,将文件存储到数据库效率更高,因为不需要获取文件描述符等其他指针.

### [6.3.1 drop、truncate和delete的区别](https://www.cnblogs.com/zhizhao/p/7825469.html)
### 6.3.2 mmap

## 6.4 缓冲池管理
缓冲池用于存储磁盘加载到内存的数据,但是存储单位是**Frame**;Frame对应磁盘上Slot的概念而不是页.
- 数据库系统必须通过一个**Page Table**(实质上是哈希结构表)来维护Frame和Page的关系.PageTable和Page  的区别:
    - Page Table是内存中的结构,维护缓冲池与PageID的映射关系;需要确保是线程安全的
    - Page Directory是数据库文件的存储结构.
    - Dirty Flag用于表示缓冲池中的数据是否被修改过.
    - Pin/Ref Count 用于表示占用该Page的线程数量
- Lock和Latch的区别:
    - Lock是数据库系统的逻辑原语,用于保护数据库的逻辑内容.
    - Latch是一种底层保护原语,用于保护数据库系统物理结构的关键部分(数据结构或者内存中的数据),Latch一般会使用自旋锁.
    - Lock的对象是**事务**, Latch的对象是线程.所以Lock保护的对象是**数据库数据**, 而Latch是**内存中的数据结构**
    - Lock发生在整个事务过程中,而Latch是产生临界资源的时候.
    - Lock主要的实现是行锁,表锁,意向锁;Latch的主要实现是操作系统级别的**读写锁和互斥量**
    - Lock的死锁检测手段一般是**等待图,依赖图或者超时机制**,Latch一般不存在死锁检测和处理机制,只是通过应用程序加锁顺序保证没有死锁的情况发生.
    - 因此Lock被**锁管理器(可以实现为一个进程,从事务接收消息并反馈)**的哈希表所容纳,Latch则在数据库系统实现的代码中.
- 如何为缓冲池分配足够的内存空间?
    - 全局策略: 针对整个系统来考虑,所做出的解决会使整个系统受益
    - 局部策略: 针对每个查询或者事务来考虑,但是对整个系统可能是糟糕的
- 多缓冲池,预读取,扫描共享,缓冲池旁路:
    - 数据库可以存在多个缓冲池,每个缓冲池都有自己的Page Table维护一套Page ID到Frame的映射.这样做是为了**可以在每个缓冲池上使用局部策略并减少Latch争用的出现.**
        - ObjectID: 使用(OnjectID,PageID,SlotNum)三元组来确定加载哪些数据,这样可以通过ObjectID查询到数据.
        - 哈希表: 通过Hash确定缓冲池的位置,通过取模确定在哪个缓冲池里
    - **预读取用于减少查询线程的停顿**,从而达到**减少最小化随机IO**.本质是预测SQL的查询范围和动作意图.
    - **扫描共享用于将一分数据尽可能多的用于多个查询线程**,这不等同于结果缓存.实质上是将多个查询线程附加到当前的**游标**数据结构中
        - 当不同的线程计算相同的数据,那么这些数据可以横跨多个线程共享它们需要的结果,这叫做**物化视图**.
        - 完整的方案只有DB2和SQL Server支持,Oracle支持的基本扫描共享技术叫**游标共享技术**,两个线程在同时执行时才会有效.
    - **缓冲池旁路是从本地内存中查找一部分想要的数据并且不污染缓冲池的缓存规律**,有的系统也叫做Buffer Cache旁路.执行查询时从磁盘将数据**加载到本地内存而不是缓冲池**,因为查询缓冲池需要Latch, 存在一定的代价,但是这样做只能是**中间结果和扫描量比较小**的时候用.
        - 许多数据库系统使用Direct IO跳过操作系统页缓存(操作系统维护的文件系统缓存)
        - 唯一利用操作系统页缓存的是PostgreSQL,并且为每个线程维护一个很小的缓冲池,因为设计者从工程师的角度去考虑,但是会降低性能.
            - PostgreSQL提供了pg_prewarm扩展程序实现当用户调用这个函数的时候就会将表的所有数据页放到缓冲池.
            - 某些数据库不喜欢操作系统页缓存是由于**跨平台特性导致不同系统的策略不同,丧失一致性**.
        - Mysql和Oracle会用所有的系统内存
        - 通过将**/proc/sys/vm/drop_caches**就可以强制让操作系统将操作系统页缓存持久化到磁盘.
- 缓冲池替换策略
    - LRU: 实现方式是跟踪Page的访问时间戳.
        - LRU-K: 只有最近被访问K次的才可以在缓冲池中.
    - Clock, 与LRU类似,使用最近未使用的算法,即逐出的页面是最近没有使用的那个.
        - 结果了LRU需要跟踪每个Page的访问时间戳的问题,但是需要维护每个Page的标志位.
        - 需要将Page设置到**环形的缓冲池*,有一个旋转的指针来检查哪些Page需要移除.
        - 不会精确的移除最近最少使用的Page.

### 原语
一般是指由若干条指令组成的程序段，用来实现某个特定功能，在执行过程中不可被中断。

### 自旋锁
自旋锁，非常浪费cpu时间，即使一个进程进不了临界区，任然会持续占有cpu
JAVA与GO中会使用TAS与Mutex结合的锁，先自旋，到达一定时间，在陷入内核
